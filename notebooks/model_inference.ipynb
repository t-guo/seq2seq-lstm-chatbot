{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/metadata_punc.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvocab_size = metadata[\"xvocab_size\"]\n",
    "w2idx = metadata[\"w2idx\"]\n",
    "idx2w = metadata[\"idx2w\"]\n",
    "emb_dim = metadata[\"emb_dim\"]\n",
    "\n",
    "start_id = w2idx[\"start_id\"]\n",
    "end_id = w2idx[\"end_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        # for chatbot, you can use the same embedding layer,\n",
    "        # for translation, you may want to use 2 separated embedding layers\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            net_encode = EmbeddingInputlayer(\n",
    "                inputs = encode_seqs,\n",
    "                vocabulary_size = xvocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'seq_embedding')\n",
    "            \n",
    "            vs.reuse_variables()\n",
    "            tl.layers.set_name_reuse(True)\n",
    "            \n",
    "            net_decode = EmbeddingInputlayer(\n",
    "                inputs = decode_seqs,\n",
    "                vocabulary_size = xvocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'seq_embedding')\n",
    "            \n",
    "        net_rnn = Seq2Seq(net_encode, net_decode,\n",
    "                cell_fn = tf.contrib.rnn.BasicLSTMCell,\n",
    "                n_hidden = emb_dim,\n",
    "                initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "                encode_sequence_length = retrieve_seq_length_op2(encode_seqs),\n",
    "                decode_sequence_length = retrieve_seq_length_op2(decode_seqs),\n",
    "                initial_state_encode = None,\n",
    "                dropout = (0.5 if is_train else None),\n",
    "                n_layer = 3,\n",
    "                return_seq_2d = True,\n",
    "                name = 'seq2seq')\n",
    "        \n",
    "        net_out = DenseLayer(net_rnn, n_units=xvocab_size, act=tf.identity, name='output')\n",
    "    return net_out, net_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] EmbeddingInputlayer model/embedding/seq_embedding: (6004, 1024)\n",
      "  [TL] EmbeddingInputlayer model/embedding/seq_embedding: (6004, 1024)\n",
      "  [**] Seq2Seq model/seq2seq: n_hidden:1024 cell_fn:BasicLSTMCell dropout:0.5 n_layer:3\n",
      "  [TL] DynamicRNNLayer model/seq2seq/seq2seq_encode: n_hidden:1024, in_dim:3 in_shape:(32, ?, 1024) cell_fn:BasicLSTMCell dropout:0.5 n_layer:3\n",
      "       batch_size (concurrent processes): 32\n",
      "  [TL] DynamicRNNLayer model/seq2seq/seq2seq_decode: n_hidden:1024, in_dim:3 in_shape:(32, ?, 1024) cell_fn:BasicLSTMCell dropout:0.5 n_layer:3\n",
      "       batch_size (concurrent processes): 32\n",
      "  [TL] DenseLayer  model/output: 6004 identity\n"
     ]
    }
   ],
   "source": [
    "# model for training\n",
    "batch_size = 32\n",
    "\n",
    "encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n",
    "decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n",
    "target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n",
    "target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") # tl.prepro.sequences_get_mask()\n",
    "net_out, _ = model(encode_seqs, decode_seqs, is_train=True, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] EmbeddingInputlayer model/embedding/seq_embedding: (6004, 1024)\n",
      "  [TL] EmbeddingInputlayer model/embedding/seq_embedding: (6004, 1024)\n",
      "  [**] Seq2Seq model/seq2seq: n_hidden:1024 cell_fn:BasicLSTMCell dropout:None n_layer:3\n",
      "  [TL] DynamicRNNLayer model/seq2seq/seq2seq_encode: n_hidden:1024, in_dim:3 in_shape:(1, ?, 1024) cell_fn:BasicLSTMCell dropout:None n_layer:3\n",
      "       batch_size (concurrent processes): 1\n",
      "  [TL] DynamicRNNLayer model/seq2seq/seq2seq_decode: n_hidden:1024, in_dim:3 in_shape:(1, ?, 1024) cell_fn:BasicLSTMCell dropout:None n_layer:3\n",
      "       batch_size (concurrent processes): 1\n",
      "  [TL] DenseLayer  model/output: 6004 identity\n"
     ]
    }
   ],
   "source": [
    "# model for inferencing\n",
    "encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "net, net_rnn = model(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\n",
    "y = tf.nn.softmax(net.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Load n_withPunc.npz SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.DenseLayer at 0x7f0caeb62890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "tl.files.load_and_assign_npz(sess=sess, name='n_withPunc.npz', network=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " seeds = [\"i can not find my luggage\", \"im stuck in yvr\", \"i want to rebook my flight\", \"where is the restroom\", \"hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query >i can not find my luggage\n",
      " >we are sorry to hear that your bag is delayed , have you filed a claim with baggage service ?\n",
      "[ 0.18993774  0.65783632  0.87407374  0.43616086  0.98127925  0.34152246\n",
      "  0.31070092  0.56916988  0.4902693   0.93209684  0.6693821   0.17464805\n",
      "  0.9933477   0.84235668  0.99366623  0.64778697  0.89538014  0.76933485\n",
      "  0.62730265  0.98704588  0.96256745]\n",
      " >please dm us your bag file number . we will take a look .\n",
      "[ 0.10109677  0.27507511  0.60650462  0.90097743  0.46034697  0.38147968\n",
      "  0.46390492  0.56463665  0.91043484  0.66198343  0.39469746  0.99943811\n",
      "  0.93408662  0.60682893  0.98128271]\n",
      " >hi unk , we are sorry to hear this . can you dm us your baggage reference , email address and contact number and we can check 1 / 2 for an update for you . thanks . 2 / 2\n",
      "[ 0.15161397  0.23477286  0.84240943  0.20274828  0.67203593  0.7384606\n",
      "  0.60005528  0.92334145  0.35418338  0.93596154  0.20327432  0.98452318\n",
      "  0.50128043  0.89411527  0.83794165  0.79068768  0.70793712  0.87134242\n",
      "  0.73759127  0.8952046   0.97819442  0.42315441  0.9867813   0.75246793\n",
      "  0.99382222  0.92710507  0.38918442  0.61632168  0.99999046  0.99395615\n",
      "  0.53886104  0.48514247  0.99805754  0.84973449  0.99987817  0.99027866\n",
      "  0.90235442  0.99415803  0.96127462  0.99999821  0.99998927  0.99129266]\n",
      " >we are sorry to hear that . you can check the status of your bags here :\n",
      "[ 0.18993774  0.65783632  0.87407374  0.43616083  0.98127925  0.34152249\n",
      "  0.31070095  0.46003467  0.80623472  0.64022213  0.48887333  0.98703384\n",
      "  0.89738941  0.9198367   0.6626063   0.43925568  0.98891515  0.9859277 ]\n",
      " >we are sorry for the delay of your bag . you can check the status of your delivery via .\n",
      "[ 0.18993774  0.65783632  0.87407374  0.43616086  0.79235852  0.36282396\n",
      "  0.7060219   0.9911629   0.82578814  0.70171219  0.55609679  0.97036856\n",
      "  0.91749799  0.69203371  0.98911822  0.52019531  0.5096671   0.60589606\n",
      "  0.66788799  0.45324486  0.99955052]\n",
      "\n",
      "Query >im stuck in yvr\n",
      " >hi , we are very sorry to hear this . we invite you to contact our reservations dept at 1 - 888 - 247 - 2262 for further assistance .\n",
      "[ 0.17972076  0.19363014  0.18620923  0.22579595  0.38227689  0.97112447\n",
      "  0.40509963  0.87910241  0.32937545  0.83503991  0.20835261  0.35715792\n",
      "  0.98684168  0.99973589  0.40399423  0.60846293  0.323125    0.44363144\n",
      "  0.48328489  0.98069578  0.99197948  0.99185914  0.99945432  0.99562079\n",
      "  0.99996471  0.99999583  0.55758309  0.2566334   0.80538404  0.95621049\n",
      "  0.99639243]\n",
      " >we are sorry for the delay in responding . we are showing an estimated departure of 8 : 30p .\n",
      "[ 0.24478078  0.36249837  0.54585844  0.50702232  0.85630518  0.27043769\n",
      "  0.3481366   0.6418125   0.6256603   0.22772504  0.20509948  0.33364829\n",
      "  0.29192266  0.9728545   0.940889    0.82209921  0.16815795  0.99939549\n",
      "  0.36575854  0.84604591  0.34865028]\n",
      " >we will get you to your destination as quickly as we can .\n",
      "[ 0.24478078  0.36249834  0.24003704  0.83638465  0.36168569  0.61207485\n",
      "  0.91720599  0.91751719  0.61737752  0.9951213   0.78135687  0.98907185\n",
      "  0.84075707  0.65415126]\n",
      " >we are sorry for the inconvenience . we will have you to your destination as soon as possible .\n",
      "[ 0.24478078  0.36249837  0.54585844  0.50702232  0.85630518  0.27043772\n",
      "  0.62359959  0.26729184  0.2554408   0.3289009   0.84877414  0.87261021\n",
      "  0.88214928  0.98678058  0.96539825  0.63835096  0.99947864  0.61244035\n",
      "  0.9804433   0.97238004]\n",
      " >hi , please contact our reservation centre at 1 - 888 - 247 - 2262 .\n",
      "[ 0.17972079  0.19363014  0.18620925  0.13465567  0.45221606  0.22799121\n",
      "  0.23265594  0.39372084  0.55170655  0.91216564  0.92454028  0.99821067\n",
      "  0.91346711  0.99998045  0.99997616  0.35940993  0.83986223]\n",
      "\n",
      "Query >i want to rebook my flight\n",
      " >please follow and dm your confirmation number for review .\n",
      "[ 0.19562773  0.36318964  0.6740129   0.9966433   0.96409059  0.95572865\n",
      "  0.39431551  0.49009535  0.91979897  0.98783898  0.99167901]\n",
      " >please follow and dm your confirmation number and i will check it for you .\n",
      "[ 0.19562773  0.36318967  0.6740129   0.9966433   0.9640907   0.95572865\n",
      "  0.39431551  0.49009523  0.69931835  0.99490297  0.56884778  0.46192664\n",
      "  0.99425679  0.99805033  0.99620408  0.99303752]\n",
      " >hi there . apologies for the inconvenience and delay . if you happen to miss your flight , you will automatically be ... 1 / 2 ... re - booked on the next available . * sd 2 / 2\n",
      "[ 0.32804954  0.20702448  0.5904665   0.51989639  0.9356246   0.97647756\n",
      "  0.47913077  0.74922395  0.56204611  0.99577194  0.49315396  0.86865175\n",
      "  0.40167877  0.9961611   0.93272173  0.952609    0.67703843  0.8942796\n",
      "  0.47684464  0.51360941  0.63336331  0.90830439  0.81717181  0.99978799\n",
      "  0.99999821  0.93604881  0.99678659  0.90559673  0.99932742  0.99594384\n",
      "  0.94858915  0.94616425  0.99340087  0.99357033  0.77572989  0.97423089\n",
      "  0.90563214  0.99998426  0.99999845  0.99994159  0.99667704]\n",
      " >hi , please dm your ticket number . i will take a look .\n",
      "[ 0.32804954  0.20702448  0.33868274  0.78189045  0.8207612   0.72935826\n",
      "  0.89995682  0.8598507   0.80982846  0.98828679  0.90725261  0.99974078\n",
      "  0.99817777  0.97573447  0.91059709]\n",
      " >hi , please follow and dm if we can assist .\n",
      "[ 0.32804954  0.20702448  0.33868274  0.78189051  0.66595978  0.98438567\n",
      "  0.92662352  0.99807966  0.99960762  0.95996797  0.99777007  0.94509208]\n",
      "\n",
      "Query >where is the restroom\n",
      " >please follow and dm your confirmation number . i will gladly check on this for you .\n",
      "[ 0.11644106  0.25106949  0.52092046  0.98296607  0.90543294  0.79798442\n",
      "  0.55254877  0.433882    0.67476159  0.72233397  0.94978732  0.85227019\n",
      "  0.42196569  0.75905097  0.97401196  0.99991024  0.99621284  0.98834604]\n",
      " >we are sorry for the delay , we are showing an estimated departure time of 8 : unk .\n",
      "[ 0.27693558  0.49699986  0.50222343  0.53735429  0.79618037  0.27532539\n",
      "  0.29065886  0.18937485  0.25438568  0.39154032  0.47667381  0.98910832\n",
      "  0.95221686  0.80921537  0.99526018  0.1202343   0.99909234  0.27178484\n",
      "  0.9147194   0.78403431]\n",
      " >we are sorry for the delay in responding , we are currently showing a departure time of 6 : 30p .\n",
      "[ 0.27693558  0.49699986  0.50222343  0.53735435  0.79618037  0.27532539\n",
      "  0.29065886  0.65445983  0.49246204  0.22136913  0.28631923  0.31715688\n",
      "  0.67545009  0.73900867  0.54394144  0.85549092  0.99818414  0.13529401\n",
      "  0.99891734  0.26381922  0.88569623  0.86479801]\n",
      " >hi unk . we are not aware of any problems with our website . we will pass on your comments to our web team .\n",
      "[ 0.12267876  0.16138251  0.83464915  0.2319167   0.37832296  0.32723099\n",
      "  0.29271531  0.98833674  0.87385851  0.49475974  0.31607127  0.35652983\n",
      "  0.47619915  0.6805262   0.13959883  0.30606639  0.39195049  0.5025267\n",
      "  0.9768734   0.61451685  0.59281534  0.77206951  0.51406962  0.71173686\n",
      "  0.75783801  0.81472421]\n",
      " >we are very sorry for any inconvenience , please continue working with an airport agent for all possible options .\n",
      "[ 0.27693558  0.49699986  0.50222343  0.96655232  0.59013963  0.82493776\n",
      "  0.42504397  0.38814336  0.19724876  0.27892771  0.90989149  0.99736625\n",
      "  0.7021904   0.75697511  0.99004877  0.74593699  0.63843066  0.41292757\n",
      "  0.95730031  0.99104828  0.99528837]\n",
      "\n",
      "Query >hi\n",
      " >we are sorry for any confusion .\n",
      "[ 0.03384854  0.56163913  0.22823985  0.36139321  0.50314415  0.7591241\n",
      "  0.41147071  0.34808889]\n",
      " >we are following you now if you would like to dm us .\n",
      "[ 0.03384854  0.56163913  0.22823985  0.99220616  0.93204081  0.329813\n",
      "  0.99146539  0.63379204  0.9901371   0.9686321   0.88648808  0.81701607\n",
      "  0.6769861   0.85084713]\n",
      " >unk , we are following you now if you would like to dm us .\n",
      "[ 0.17028029  0.57131106  0.17544401  0.43631345  0.32264623  0.98477733\n",
      "  0.89519471  0.34890646  0.99023205  0.63601309  0.99245691  0.97093379\n",
      "  0.89294732  0.801732    0.61287522  0.81030643]\n",
      " >unk . we are following you now .\n",
      "[ 0.17028029  0.57131106  0.19653586  0.45992112  0.39900631  0.98915601\n",
      "  0.89555669  0.29978254  0.65395361]\n",
      " >unk , please dm us your booking reference and email address . thanks .\n",
      "[ 0.17028029  0.57131106  0.17544401  0.31729522  0.53499079  0.85558772\n",
      "  0.68342429  0.76780802  0.39360073  0.678581    0.56868708  0.44440222\n",
      "  0.43403715  0.58240831  0.98971117]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    print \"Query >\" + seed\n",
    "    seed_id = [w2idx[w] for w in seed.split(\" \") if w in w2idx.keys()]\n",
    "    \n",
    "    for _ in range(5):  # 1 Query --> 5 Reply\n",
    "        # 1. encode, get state\n",
    "        state = sess.run(net_rnn.final_state_encode, {encode_seqs2: [seed_id]})\n",
    "        # 2. decode, feed start_id, get first word\n",
    "        #   ref https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py\n",
    "        o, state = sess.run([y, net_rnn.final_state_decode], {net_rnn.initial_state_decode: state, \n",
    "                                                              decode_seqs2: [[start_id]]})\n",
    "        w_id = tl.nlp.sample_top(o[0], top_k=3)\n",
    "        w = idx2w[w_id]\n",
    "        \n",
    "        # sort and save probabilities\n",
    "        probs = []\n",
    "        probabilities = o[0][w_id]\n",
    "        \n",
    "        # this stores the probability of the top word each time\n",
    "        probs = np.append(probs, probabilities)\n",
    "        \n",
    "        # 3. decode, feed state iteratively\n",
    "        sentence = [w]\n",
    "        for _ in range(500): # max sentence length\n",
    "            o, state = sess.run([y, net_rnn.final_state_decode], {net_rnn.initial_state_decode: state,\n",
    "                                                                  decode_seqs2: [[w_id]]})\n",
    "            w_id = tl.nlp.sample_top(o[0], top_k=2)\n",
    "            w = idx2w[w_id]\n",
    "            \n",
    "            # sort and save probabilities\n",
    "            probabilities = np.sort(o[0])\n",
    "            probabilities = probabilities[::-1]\n",
    "            \n",
    "            probs = np.append(probs, probabilities[0:1])\n",
    "            \n",
    "            if w_id == end_id:\n",
    "                break\n",
    "            sentence = sentence + [w]\n",
    "        print \" >\" + ' '.join(sentence)\n",
    "        print(probs)\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
